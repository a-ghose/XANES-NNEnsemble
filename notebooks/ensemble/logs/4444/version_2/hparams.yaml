activation: relu
batch_norm: true
before_latent_activation: sigmoid
criterion: mse
dropout: 0.0
hidden_sizes:
- 39
- 51
- 57
- 37
- 61
- 95
- 68
- 84
- 82
- 85
input_size: 155
last_batch_norm: false
output_size: 200
